# AdaLoRAâ€‘Drop: Parameterâ€‘Efficient RoBERTa Fineâ€‘Tuning on SICKâ€‘E

> **TL;DR**: We combine AdaLoRAâ€™s adaptive rank allocation with LoRAâ€‘Drop style layer sharing to fineâ€‘tune RoBERTaâ€‘base for textual entailment (SICKâ€‘E) while training **<0.1%** of the modelâ€™s parameters, with only a small accuracy drop vs. full fineâ€‘tuning.

---

## ğŸ§  Motivation
Full model fineâ€‘tuning of RoBERTaâ€‘base (~125M params) is computeâ€‘ and memoryâ€‘heavyâ€”overkill for a small dataset like **SICKâ€‘E (~9.8k pairs)**. Adapterâ€‘style methods (LoRA/AdaLoRA) update a tiny set of weights while keeping the backbone frozen. Our approach, **AdaLoRAâ€‘Drop**, pushes parameter efficiency further by **(1)** adapting rank per layer (AdaLoRA) and **(2)** **sharing** adapter modules across lowâ€‘importance layers (inspired by LoRAâ€‘Dropâ€™s Î”Wxâ€‘based importance).

---

## âœ¨ Key ideas
- **Twoâ€‘stage procedure**
  1) **Importance estimation** on a small data slice using Î”Wx energy; sort layers and mark a minimal set that covers 90â€“95% cumulative importance as **highâ€‘importance**.
  2) **Fineâ€‘tuning with AdaLoRA**: give **individual** AdaLoRA modules to highâ€‘importance layers; **share one module per shape group** across the remaining lowâ€‘importance layers.
- **Tunable budget** via the cumulative importance threshold (e.g., 95% vs. 90%).
- **Edgeâ€‘friendly**: tiny adapter sizes cut memory/latency and enable cheaper GPUs or onâ€‘device scenarios.

---

## ğŸ“Š Results (SICKâ€‘E test accuracy vs. trainable params)

| Model | Trainable Params | Test Acc. |
|---|---:|---:|
| Full fineâ€‘tune (RoBERTaâ€‘base) | 124,647,939 | 0.8863 |
| LoRAâ€‘all | 1,256,451 | 0.8939 |
| LoRAâ€‘Drop@95 | 1,062,915 | 0.8900 |
| AdaLoRAâ€‘all | 682,272 | 0.8805 |
| **AdaLoRAâ€‘Drop@95 (ours)** | **133,698** | **0.8744** |
| **AdaLoRAâ€‘Drop@90 (ours)** | **66,846** | **0.8736** |

> **Takeaway**: **AdaLoRAâ€‘Drop@90** achieves a **>99.9% reduction** in trainable parameters with only ~1â€“1.5% absolute accuracy drop vs. full fineâ€‘tuning.

---

## ğŸ“¦ Repository structure
```
.
â”œâ”€â”€ ada-drop.ipynb                     # Endâ€‘toâ€‘end notebook: importance scoring + AdaLoRAâ€‘Drop FT
â”œâ”€â”€ slides/                            # Overview deck
â”‚   â””â”€â”€ AdaLoRA-Drop-Parameter-Efficient-RoBERTa-Fine-Tuning-on-SICK-E.pptx
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data.py                        # SICKâ€‘E dataset loading utilities
â”‚   â”œâ”€â”€ modeling.py                    # LoRA/AdaLoRA adapters & sharing logic
â”‚   â”œâ”€â”€ importance.py                  # Î”Wx importance estimation
â”‚   â”œâ”€â”€ train.py                       # CLI training entrypoint (optional)
â”‚   â””â”€â”€ eval.py                        # Evaluation utilities
â”œâ”€â”€ requirements.txt                   # Python deps
â””â”€â”€ README.md
```
*Note:* Only the notebook and slides are included initially; `src/` and scripts are optional helpers if you want a scriptified version.

---

## ğŸš€ Quickstart

### 1) Environment
- Python â‰¥ 3.10
- PyTorch (CUDA recommended)
- `transformers`, `datasets`, `accelerate`
- `peft` (for LoRA/AdaLoRA), `scikit-learn`, `tqdm`

Create an environment and install:
```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -U pip wheel
pip install torch torchvision torchaudio  # choose the right CUDA build for your system
pip install transformers datasets accelerate peft scikit-learn tqdm
```

### 2) Data: SICKâ€‘E (local files)
Place the three SICK files under `data/` exactly as follows:

- `data/SICK_train.txt`
- `data/SICK_test.txt`
- `data/SICK_trial.txt`

They are **tab-separated** with headers `pair_ID, sentence_A, sentence_B, relatedness_score, entailment_judgment`.

Example loader (pandas):
```python
import pandas as pd
train = pd.read_csv('data/SICK_train.txt', sep='	')
trial = pd.read_csv('data/SICK_trial.txt', sep='	')
test  = pd.read_csv('data/SICK_test.txt',  sep='	')
```

Or build a ğŸ¤— Datasets dataset from the TSV files:
```python
from datasets import DatasetDict, Dataset
import pandas as pd
train = Dataset.from_pandas(pd.read_csv('data/SICK_train.txt', sep='	'))
trial = Dataset.from_pandas(pd.read_csv('data/SICK_trial.txt', sep='	'))
test  = Dataset.from_pandas(pd.read_csv('data/SICK_test.txt',  sep='	'))
raw = DatasetDict({
    'train': train,
    'validation': trial,  # SICK provides a trial/dev split
    'test': test
})
```

> If you prefer the hosted copy, you can still use `datasets.load_dataset("sick")`, but the notebook is set up to read the **local** files by default.

### 3) Run the notebook
Open **`ada-drop.ipynb`** and run all cells. The notebook covers:
- Importance estimation on a small subset (e.g., ~10% of train split)
- Layer ranking & thresholding (90% / 95%)
- AdaLoRAâ€‘Drop fineâ€‘tuning (target rank small, e.g., r=4)
- Evaluation on the SICKâ€‘E test set

### 4) (Optional) Scripted training
If you prefer CLI over notebooks, adapt the provided `src/` stubs:
```bash
python -m src.train \
  --model roberta-base \
  --dataset sick \
  --epochs 5 \
  --batch_size 64 \
  --lr 2e-4 \
  --adalora_target_rank 4 \
  --importance_threshold 0.90  # or 0.95
```

---

## âš™ï¸ Method details

### Stage 1 â€” Importance estimation
- Attach lowâ€‘rank adapters with small rank to targeted layers.
- Train briefly (e.g., 3 epochs on ~10% of SICKâ€‘E) to gather **Î”Wx** statistics.
- Compute perâ€‘layer importance \(I_i \propto \mathbb{E}[\lVert \Delta W_i x \rVert^2]\) and sort.
- Choose layers that cover **90â€“95%** cumulative importance as highâ€‘importance.

### Stage 2 â€” AdaLoRAâ€‘Drop fineâ€‘tuning
- Reset a fresh **RoBERTaâ€‘base** classifier (`num_labels=3`).
- Apply **AdaLoRA** with a small target rank (e.g., `r=4`).
- **Highâ€‘importance** layers â†’ independent AdaLoRA modules.
- **Lowâ€‘importance** layers â†’ **share one module per shape group**.
- Train ~5 epochs with early stopping by validation accuracy.

---

## ğŸ”¬ Reproducing the table
- Use the notebookâ€™s experiment grid to toggle: **Full FT**, **LoRAâ€‘all**, **LoRAâ€‘Drop@95**, **AdaLoRAâ€‘all**, **AdaLoRAâ€‘Drop@95/90**.
- Track **trainable parameter counts** by summing adapter parameters only.
- Report **test accuracy** on SICKâ€‘E.

---

## ğŸ“ Notes & limitations
- Currently evaluated only on **SICKâ€‘E** with **RoBERTaâ€‘base**.
- Î”Wx energy is a simple importance proxy; other signals (e.g., gradientâ€‘based) might further improve selection.

---

## ğŸ“š References
- **LoRA Without Regret**
- **AdaLoRA: Adaptive Budget Allocation for Parameterâ€‘Efficient Fineâ€‘Tuning**
- **LoRAâ€‘Drop: Efficient LoRA Parameter Pruning based on Output Evaluation**

(See the slides for concise conceptual diagrams and comparisons.)

---

## ğŸ“ License
MIT

---

## ğŸ™Œ Acknowledgments
- Built on ğŸ¤— Transformers & Datasets, PEFT, and PyTorch.
- Inspired by LoRA/AdaLoRA/LoRAâ€‘Drop lines of work.

---

## ğŸ¤ How to cite
```bibtex
@software{adalora_drop_sicke_2025,
  title        = {AdaLoRAâ€‘Drop: Parameterâ€‘Efficient RoBERTa Fineâ€‘Tuning on SICKâ€‘E},
  author       = {Your Name},
  year         = {2025},
  url          = {https://github.com/<yourâ€‘org>/adalora-drop-roberta-sicke}
}
```

