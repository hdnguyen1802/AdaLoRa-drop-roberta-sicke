{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13762796,"sourceType":"datasetVersion","datasetId":8758368}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets peft accelerate evaluate peft\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:28:10.846714Z","iopub.execute_input":"2025-11-17T09:28:10.846920Z","iopub.status.idle":"2025-11-17T09:29:28.590963Z","shell.execute_reply.started":"2025-11-17T09:28:10.846902Z","shell.execute_reply":"2025-11-17T09:29:28.589992Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =========================\n# 0) SETUP & INSTALLS\n# =========================\n\nimport os, math, json, random, functools, collections\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\nfrom peft import AdaLoraConfig, get_peft_model, TaskType\nfrom transformers import DataCollatorWithPadding\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED = 7\nrandom.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:29:28.595244Z","iopub.execute_input":"2025-11-17T09:29:28.595536Z","iopub.status.idle":"2025-11-17T09:29:59.808779Z","shell.execute_reply.started":"2025-11-17T09:29:28.595509Z","shell.execute_reply":"2025-11-17T09:29:59.808169Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 09:29:42.568758: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763371782.734490      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763371782.783358      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\n\nmodel_name = \"roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n\n# =========================\n# SICK-E from Kaggle files\n# =========================\ndata_files = {\n    \"train\": \"/kaggle/input/sick-e/SICK_train.txt\",\n    \"validation\": \"/kaggle/input/sick-e/SICK_trial.txt\",  # use trial as dev\n    \"test\": \"/kaggle/input/sick-e/SICK_test.txt\",\n}\n\n# The files are tab-separated with a header row\nraw = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n\n# 3-way NLI labels\nlabel2id = {\"ENTAILMENT\": 0, \"NEUTRAL\": 1, \"CONTRADICTION\": 2}\nid2label = {v: k for k, v in label2id.items()}\n\ndef preprocess(example):\n    # example keys: \"sentence_A\", \"sentence_B\", \"entailment_judgment\", ...\n    enc = tokenizer(\n        example[\"sentence_A\"],\n        example[\"sentence_B\"],\n        truncation=True,\n        max_length=256,\n    )\n    # handle batched=True\n    labels = example[\"entailment_judgment\"]\n    if isinstance(labels, list):\n        enc[\"label\"] = [label2id[l] for l in labels]\n    else:\n        enc[\"label\"] = label2id[labels]\n    return enc\n\ntrain_ds = raw[\"train\"].map(preprocess, batched=True)\nvalid_ds = raw[\"validation\"].map(preprocess, batched=True)\ntest_ds  = raw[\"test\"].map(preprocess, batched=True)\n\ncols = [\"input_ids\", \"attention_mask\", \"label\"]\ntrain_ds.set_format(type=\"torch\", columns=cols)\nvalid_ds.set_format(type=\"torch\", columns=cols)\ntest_ds.set_format(type=\"torch\", columns=cols)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:29:59.809605Z","iopub.execute_input":"2025-11-17T09:29:59.810250Z","iopub.status.idle":"2025-11-17T09:30:03.149014Z","shell.execute_reply.started":"2025-11-17T09:29:59.810230Z","shell.execute_reply":"2025-11-17T09:30:03.148245Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b8c49bed44848d4b25c31eeea880ec8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b80bf9f480d34849a68052d98635ceb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b514e478e4d467f82b99f324150f2eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cb79fa9474747aaadb945db4b882a1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc6b20d0b5194b2cb00a03fc9406e05e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00bf5260509b4c578fcd39149ea9664e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d70a951735c749d39f9b370fe3c26279"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ef3fa5388c46cdb6aa25bdadad042b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acd104cef1b841818df67cb3871e13ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0cb5a107da742cdaf80b6d2a360573b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4927 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc08ab22e1024c92a87f8a588c70bdd6"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# =========================\n# 2) TARGET MODULES (All attention + FFN)\n#    Wq, Wk, Wv, Wo, Wf1, Wf2\n# =========================\n# We'll rely on module name patterns in RoBERTa:\n#  roberta.encoder.layer.{i}.attention.self.{query,key,value}\n#  roberta.encoder.layer.{i}.attention.output.dense       -> Wo\n#  roberta.encoder.layer.{i}.intermediate.dense           -> Wf1\n#  roberta.encoder.layer.{i}.output.dense (NOT attention) -> Wf2\n\nTARGET_PATTERNS = [\"attention.self.query\",\n                   \"attention.self.key\",\n                   \"attention.self.value\",\n                   \"attention.output.dense\",  # Wo\n                   \"intermediate.dense\",      # Wf1\n                   \"output.dense\"]            # Wf2 (feedforward output)\n\n# Helper to test if a module full name is one we want (and classify its shape-group)\ndef classify_module_name(full_name):\n    # returns (group_key, pretty_tag) or (None, None)\n    # group_key used for sharing; pretty_tag displayed to user\n    if \".attention.self.query\" in full_name:\n        return (\"Wq\", \"Wq\")\n    if \".attention.self.key\" in full_name:\n        return (\"Wk\", \"Wk\")\n    if \".attention.self.value\" in full_name:\n        return (\"Wv\", \"Wv\")\n    if \".attention.output.dense\" in full_name:\n        return (\"Wo\", \"Wo\")\n    # Now, distinguish FFN output.dense vs attention.output.dense\n    if \".intermediate.dense\" in full_name:\n        return (\"Wf1\", \"Wf1\")\n    if \".output.dense\" in full_name and \".attention.\" not in full_name:\n        return (\"Wf2\", \"Wf2\")\n    return (None, None)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:30:03.150163Z","iopub.execute_input":"2025-11-17T09:30:03.150362Z","iopub.status.idle":"2025-11-17T09:30:03.156133Z","shell.execute_reply.started":"2025-11-17T09:30:03.150347Z","shell.execute_reply":"2025-11-17T09:30:03.155391Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import math\n\n# =========================\n# SCHEDULE CONSTANTS\n# =========================\n# Stage-1 (importance) hyperparams\nALPHA_STAGE1      = 0.10    # 10% subset of train\nEPOCHS_STAGE1     = 3\nBATCH_SIZE_STAGE1 = 64\n\n# Stage-2 (full training) hyperparams\nEPOCHS_STAGE2     = 5\nBATCH_SIZE_STAGE2 = 64\n\nN_train = len(train_ds)\n\n# ---------- Stage-1 schedule ----------\nK_STAGE1 = max(1, int(ALPHA_STAGE1 * N_train))\nSTEPS_PER_EPOCH_STAGE1 = math.ceil(K_STAGE1 / BATCH_SIZE_STAGE1)\nTOTAL_STEP_STAGE1      = EPOCHS_STAGE1 * STEPS_PER_EPOCH_STAGE1\n\nTINIT_STAGE1  = int(round(0.15 * TOTAL_STEP_STAGE1))\nTMID_STAGE1   = int(round(0.40 * TOTAL_STEP_STAGE1))\nTFINAL_STAGE1 = TOTAL_STEP_STAGE1 - TINIT_STAGE1 - TMID_STAGE1\n\nprint(\"[Stage-1] N_train:\", N_train)\nprint(\"[Stage-1] subset K:\", K_STAGE1)\nprint(\"[Stage-1] steps/epoch:\", STEPS_PER_EPOCH_STAGE1)\nprint(\"[Stage-1] total_step:\", TOTAL_STEP_STAGE1)\nprint(\"[Stage-1] tinit:\", TINIT_STAGE1, \"tfinal:\", TFINAL_STAGE1, \"middle:\", TMID_STAGE1)\n\n# ---------- Stage-2 schedule ----------\nSTEPS_PER_EPOCH_STAGE2 = math.ceil(N_train / BATCH_SIZE_STAGE2)\nTOTAL_STEP_STAGE2      = EPOCHS_STAGE2 * STEPS_PER_EPOCH_STAGE2\n\nTINIT_STAGE2  = int(round(0.15 * TOTAL_STEP_STAGE2))\nTMID_STAGE2   = int(round(0.40 * TOTAL_STEP_STAGE2))\nTFINAL_STAGE2 = TOTAL_STEP_STAGE2 - TINIT_STAGE2 - TMID_STAGE2\n\nprint(\"[Stage-2] steps/epoch:\", STEPS_PER_EPOCH_STAGE2)\nprint(\"[Stage-2] total_step:\", TOTAL_STEP_STAGE2)\nprint(\"[Stage-2] tinit:\", TINIT_STAGE2, \"tfinal:\", TFINAL_STAGE2, \"middle:\", TMID_STAGE2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:30:03.158713Z","iopub.execute_input":"2025-11-17T09:30:03.158961Z","iopub.status.idle":"2025-11-17T09:30:03.181222Z","shell.execute_reply.started":"2025-11-17T09:30:03.158921Z","shell.execute_reply":"2025-11-17T09:30:03.180473Z"}},"outputs":[{"name":"stdout","text":"[Stage-1] N_train: 4500\n[Stage-1] subset K: 450\n[Stage-1] steps/epoch: 8\n[Stage-1] total_step: 24\n[Stage-1] tinit: 4 tfinal: 10 middle: 10\n[Stage-2] steps/epoch: 71\n[Stage-2] total_step: 355\n[Stage-2] tinit: 53 tfinal: 160 middle: 142\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# =========================\n# ADALORA CONFIGS\n# =========================\nADALORA_STAGE1_CFG = dict(\n    target_r = 4,\n    init_r   = 6,\n    lora_alpha = 16,\n    lora_dropout = 0.0,\n    beta1 = 0.85, beta2 = 0.85,\n    tinit = TINIT_STAGE1,\n    tfinal = TFINAL_STAGE1,\n    deltaT = 1,\n    orth_reg_weight = 0.3,\n    total_step = TOTAL_STEP_STAGE1,\n)\n\nADALORA_STAGE2_CFG = dict(\n    target_r = 4,\n    init_r   = 6,\n    lora_alpha = 16,\n    lora_dropout = 0.0,\n    beta1 = 0.85, beta2 = 0.85,\n    tinit = TINIT_STAGE2,\n    tfinal = TFINAL_STAGE2,\n    deltaT = 1,\n    orth_reg_weight = 0.3,\n    total_step = TOTAL_STEP_STAGE2,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:30:03.181878Z","iopub.execute_input":"2025-11-17T09:30:03.182088Z","iopub.status.idle":"2025-11-17T09:30:03.201303Z","shell.execute_reply.started":"2025-11-17T09:30:03.182072Z","shell.execute_reply":"2025-11-17T09:30:03.200461Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# =========================\n# 4) STAGE-1 MODEL: build w/ AdaLoRA on all target modules\n# =========================\ndef build_stage1_model():\n    base = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n    base.to(device)\n    # Use broad patterns (we filter later when we compute sharing)\n    peft_cfg = AdaLoraConfig(\n        task_type=TaskType.SEQ_CLS,\n        target_modules=[\"query\",\"key\",\"value\",\"dense\"],  # these substrings cover all six\n        **ADALORA_STAGE1_CFG\n    )\n    peft_model = get_peft_model(base, peft_cfg)\n    peft_model.print_trainable_parameters()\n    return peft_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:30:03.202166Z","iopub.execute_input":"2025-11-17T09:30:03.202375Z","iopub.status.idle":"2025-11-17T09:30:03.218671Z","shell.execute_reply.started":"2025-11-17T09:30:03.202358Z","shell.execute_reply":"2025-11-17T09:30:03.217900Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# =========================\n# 5) STAGE-1 TRAINING on α=0.1 subset + IMPORTANCE via ||ΔW x||^2\n#    Implement LoRA-drop importance with AdaLoRA adapters\n# =========================\n# We'll register forward-pre hooks on every AdaLoRA-wrapped Linear and compute\n#  delta = scaling * B(A(x))  (and include AdaLoRA magnitude vector if present)\n# Accumulate per-module energy: g_i += ||delta||^2, then normalize to importances I_i\n# See LoRA-drop Sec. 3.1/3.2 for this criterion. (We use AdaLoRA to generate Δ.) \n\n\n# --- helper: detect AdaLoRA-wrapped linear-like modules ---\ndef is_adalora_linear(m):\n    \"\"\"\n    PEFT's AdaLoRA wraps target layers in SVDLinear, which subclasses\n    nn.Module + AdaLoraLayer (not nn.Linear). We detect them by the\n    presence of lora_A / lora_B ParameterDicts.\n    \"\"\"\n    return hasattr(m, \"lora_A\") and hasattr(m, \"lora_B\")\n\ndef make_energy_meter(model):\n    \"\"\"\n    Register forward-pre hooks on every AdaLoRA/LoRA layer and accumulate\n    an importance score per module:\n        g_i += || ΔW_i x ||^2  over the warm-up mini-batches.\n\n    For AdaLoRA: ΔW = B diag(E) A\n    For LoRA:    ΔW = B A              (no E)\n    \"\"\"\n    energy = collections.defaultdict(float)\n    hooks = []\n\n    def register_one(full_name, module):\n        # classify into Wq/Wk/Wv/Wo/Wf1/Wf2 groups for reporting\n        group, tag = classify_module_name(full_name)\n        key = (full_name, group or \"misc\", tag or \"misc\")\n\n        def pre_hook(mod, inputs):\n            x = inputs[0]        # shape: [batch, seq, hidden] or [*, hidden]\n            # flatten to 2D: [N, in_features] so F.linear works\n            x2 = x.view(-1, x.size(-1))\n\n            # 1) pick an adapter name robustly\n            if not hasattr(mod, \"lora_A\") or len(mod.lora_A) == 0:\n                return\n\n            active = getattr(mod, \"active_adapter\", None)\n            if isinstance(active, str) and active in mod.lora_A:\n                adapter_name = active\n            else:\n                adapter_name = next(iter(mod.lora_A.keys()))\n\n            if adapter_name not in mod.lora_B:\n                return  # adapter not fully attached\n\n            # 2) Get A and B; they can be nn.Linear or Tensors/Parameters\n            A_obj = mod.lora_A[adapter_name]\n            B_obj = mod.lora_B[adapter_name]\n\n            # unwrap .weight if these are Linear modules (LoRA case)\n            A = A_obj.weight if isinstance(A_obj, nn.Linear) else A_obj\n            B = B_obj.weight if isinstance(B_obj, nn.Linear) else B_obj\n\n            # low-rank projection: x2 -> rank space\n            # F.linear(x, weight) = x @ weight.T, so weight=[r, in_dim]\n            h = F.linear(x2, A)   # [N, r]\n\n            # 3) AdaLoRA-specific singular values E (absent for plain LoRA)\n            if hasattr(mod, \"lora_E\") and adapter_name in mod.lora_E:\n                E = mod.lora_E[adapter_name].view(-1)   # [r]\n                h = h * E                               # broadcast over batch\n\n            # 4) project back to output space with B: [out_dim, r]\n            delta = F.linear(h, B)  # [N, out_dim]\n\n            # 5) LoRA / AdaLoRA scaling (α / r etc.), stored in mod.scaling\n            if hasattr(mod, \"scaling\") and adapter_name in mod.scaling:\n                delta = delta * mod.scaling[adapter_name]\n\n            # accumulate energy ||ΔW x||^2 for this module\n            e = delta.detach().pow(2).sum().item()\n            energy[key] += e\n\n        hooks.append(module.register_forward_pre_hook(pre_hook, with_kwargs=False))\n\n    # attach hooks to all modules that have LoRA/AdaLoRA layers\n    for full_name, module in model.named_modules():\n        if is_adalora_linear(module):   # this still just checks for lora_A/lora_B\n            register_one(full_name, module)\n\n    return energy, hooks\n\n\n\ndef remove_hooks(hooks):\n    for h in hooks: h.remove()\n\ndef train_for_importance(\n    peft_model,\n    alpha=ALPHA_STAGE1,\n    epochs=EPOCHS_STAGE1,\n    lr=1.2e-3,\n    batch_size=BATCH_SIZE_STAGE1,\n):\n    # sub-sample α of train set\n    n = len(train_ds)\n    k = max(1, int(alpha * n))\n    idx = list(range(n))\n    random.shuffle(idx); idx = idx[:k]\n    small = train_ds.select(idx)\n\n    dl = DataLoader(\n        small,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=data_collator,\n        drop_last=False,\n    )\n\n    # Only AdaLoRA parameters train (base already frozen by PEFT)\n    unique_params = {}\n    for n, p in peft_model.named_parameters():\n        if p.requires_grad:\n            unique_params[id(p)] = p\n    optim = torch.optim.AdamW(list(unique_params.values()), lr=lr)\n\n    total_steps = epochs * math.ceil(len(dl))\n    sched = get_linear_schedule_with_warmup(\n        optim,\n        num_warmup_steps=int(0.1 * total_steps),\n        num_training_steps=total_steps,\n    )\n\n    energy, hooks = make_energy_meter(peft_model)\n    peft_model.train()\n\n    global_step = 0\n\n    for ep in range(epochs):\n        for batch in dl:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = peft_model(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                labels=batch[\"labels\"],\n            )\n            loss = out.loss\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(peft_model.parameters(), 1.0)\n            optim.step()\n\n            # AdaLoRA rank allocation in Stage-1\n            global_step += 1\n            peft_model.base_model.update_and_allocate(global_step)\n\n            sched.step()\n            optim.zero_grad()\n\n    remove_hooks(hooks)\n\n    # Normalize importances\n    total_e = sum(energy.values()) + 1e-12\n    importance = []\n    for (full_name, group, tag), val in energy.items():\n        importance.append({\n            \"name\": full_name, \"group\": group, \"tag\": tag,\n            \"energy\": val, \"importance\": val / total_e,\n        })\n    importance.sort(key=lambda d: d[\"importance\"], reverse=True)\n    return importance\n\n\n# Run Stage-1 (warm-up + importance)\nstage1_model = build_stage1_model()\nimportance = train_for_importance(stage1_model, alpha=0.10, epochs=5, lr=1.2e-3, batch_size=64)\n\ndef split_importance(importance, threshold):\n    cum, important_ids, low_ids = 0.0, [], []\n    for item in importance:\n        if cum < threshold:\n            important_ids.append(item[\"name\"])\n            cum += item[\"importance\"]\n        else:\n            low_ids.append(item[\"name\"])\n    return important_ids, low_ids, cum\n\n# Use the same importance list for multiple thresholds\nimportant95_ids, low95_ids, cum95 = split_importance(importance, 0.95)\nimportant90_ids, low90_ids, cum90 = split_importance(importance, 0.90)\n\nprint(f\"[Stage-1@0.95] selected {len(important95_ids)} important, {len(low95_ids)} low; cum={cum95:.3f}\")\nprint(f\"[Stage-1@0.90] selected {len(important90_ids)} important, {len(low90_ids)} low; cum={cum90:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:30:03.219627Z","iopub.execute_input":"2025-11-17T09:30:03.220021Z","iopub.status.idle":"2025-11-17T09:30:19.101769Z","shell.execute_reply.started":"2025-11-17T09:30:03.220001Z","shell.execute_reply":"2025-11-17T09:30:19.100947Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b7562cc17844adebabcde03a6006bb5"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,588,659 || all params: 126,236,670 || trainable%: 1.2585\n[Stage-1@0.95] selected 32 important, 40 low; cum=0.952\n[Stage-1@0.90] selected 26 important, 46 low; cum=0.907\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Run Stage-1 (warm-up + importance)\nstage1_model = build_stage1_model()\nimportance = train_for_importance(stage1_model, alpha=0.10, epochs=5, lr=1.2e-3, batch_size=64)\n\ndef split_importance(importance, threshold):\n    cum, important_ids, low_ids = 0.0, [], []\n    for item in importance:\n        if cum < threshold:\n            important_ids.append(item[\"name\"])\n            cum += item[\"importance\"]\n        else:\n            low_ids.append(item[\"name\"])\n    return important_ids, low_ids, cum\n\n# Use the same importance list for multiple thresholds\nimportant95_ids, low95_ids, cum95 = split_importance(importance, 0.95)\nimportant90_ids, low90_ids, cum90 = split_importance(importance, 0.90)\n\nprint(f\"[Stage-1@0.95] selected {len(important95_ids)} important, {len(low95_ids)} low; cum={cum95:.3f}\")\nprint(f\"[Stage-1@0.90] selected {len(important90_ids)} important, {len(low90_ids)} low; cum={cum90:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:30:19.102653Z","iopub.execute_input":"2025-11-17T09:30:19.102966Z","iopub.status.idle":"2025-11-17T09:30:31.798488Z","shell.execute_reply.started":"2025-11-17T09:30:19.102923Z","shell.execute_reply":"2025-11-17T09:30:31.797505Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,588,659 || all params: 126,236,670 || trainable%: 1.2585\n[Stage-1@0.95] selected 29 important, 43 low; cum=0.952\n[Stage-1@0.90] selected 23 important, 49 low; cum=0.905\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# =========================\n# 6) STAGE-2 MODEL: rebuild, then tie low-importance modules\n#     to one shared AdaLoRA PER SHAPE GROUP\n# =========================\ndef build_stage2_model():\n    base = AutoModelForSequenceClassification.from_pretrained(\n        model_name, num_labels=3\n    ).to(device)\n    peft_cfg = AdaLoraConfig(\n        task_type=TaskType.SEQ_CLS,\n        target_modules=[\"query\", \"key\", \"value\", \"dense\"],  # cover all six types\n        **ADALORA_STAGE2_CFG\n    )\n    model = get_peft_model(base, peft_cfg)\n    return model\n\nstage2_model = build_stage2_model()\n\n# Create a shared-parameter bank per group (Wq, Wk, Wv, Wo, Wf1, Wf2)\nclass SharedAdaLoraBank(nn.Module):\n    \"\"\"\n    For each group (Wq/Wk/Wv/Wo/Wf1/Wf2), create a single AdaLoRA\n    parameter triple (A, B, E) and tie all LOW-IMPORTANCE modules\n    in that group to share them.\n    \"\"\"\n    def __init__(self, model, adapter_name=\"default\"):\n        super().__init__()\n        self.adapter_name = adapter_name\n\n        # Discover one exemplar per group to read shapes from\n        exemplars = {}\n        for name, m in model.named_modules():\n            group, tag = classify_module_name(name)\n            if group and group not in exemplars and is_adalora_linear(m) and len(m.lora_A) > 0:\n                exemplars[group] = m\n\n        # Build shared A/B/E parameters for each group\n        for group, m in exemplars.items():\n            # robustly pick adapter name\n            adapter = self.adapter_name\n            if adapter not in m.lora_A:\n                adapter = next(iter(m.lora_A.keys()))\n\n            A = m.lora_A[adapter]  # shape [r, in_dim]\n            B = m.lora_B[adapter]  # shape [out_dim, r]\n            r, in_dim = A.shape\n            out_dim, r2 = B.shape\n            assert r == r2\n\n            # register shared A and B\n            A_param = nn.Parameter(torch.empty_like(A))\n            B_param = nn.Parameter(torch.empty_like(B))\n            # init: A ~ Kaiming, B ~ 0 (standard LoRA-style)\n            nn.init.kaiming_uniform_(A_param, a=math.sqrt(5))\n            nn.init.zeros_(B_param)\n\n            self.register_parameter(f\"{group}_A\", A_param)\n            self.register_parameter(f\"{group}_B\", B_param)\n\n            # AdaLoRA-specific singular values E (if present)\n            if hasattr(m, \"lora_E\") and adapter in m.lora_E:\n                E = m.lora_E[adapter]\n                E_param = nn.Parameter(torch.zeros_like(E))  # start at 0\n            else:\n                E_param = nn.Parameter(torch.zeros(r, 1))\n            self.register_parameter(f\"{group}_E\", E_param)\n\n    def tie_into(self, module, group):\n        \"\"\"\n        Replace this module's local A/B/E Parameters with the shared\n        ones for its group.\n        \"\"\"\n        adapter = self.adapter_name\n        if adapter not in module.lora_A:\n            adapter = next(iter(module.lora_A.keys()))\n\n        # share A, B, and E\n        module.lora_A[adapter] = getattr(self, f\"{group}_A\")\n        module.lora_B[adapter] = getattr(self, f\"{group}_B\")\n        if hasattr(module, \"lora_E\"):\n            module.lora_E[adapter] = getattr(self, f\"{group}_E\")\n\n\n# instantiate bank and attach to the PEFT model so its params are counted\nshared_bank = SharedAdaLoraBank(stage2_model).to(device)\nstage2_model.shared_bank = shared_bank  # important: registers as submodule\n\n\n# Apply tying for LOW-IMPORTANCE modules: share A/B/E within each group\ndef apply_sharing_with_bank(model, low_module_names, bank):\n    \"\"\"\n    Tie all LOW-IMPORTANCE modules in low_module_names to the given bank.\n    \"\"\"\n    name2module = {n: m for n, m in model.named_modules()}\n    applied = 0\n    for n in low_module_names:\n        if n in name2module and is_adalora_linear(name2module[n]):\n            group, tag = classify_module_name(n)\n            if group is None:\n                continue\n            bank.tie_into(name2module[n], group)\n            applied += 1\n    print(f\"[Stage-2] Shared parameters tied across {applied} low-importance modules.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:30:31.799309Z","iopub.execute_input":"2025-11-17T09:30:31.799510Z","iopub.status.idle":"2025-11-17T09:30:32.268291Z","shell.execute_reply.started":"2025-11-17T09:30:31.799489Z","shell.execute_reply":"2025-11-17T09:30:32.267352Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# =========================\n# 8) STAGE-2 TRAIN/VALID/TEST\n# =========================\ndef compute_adalora_budget(model, adapter_name=\"default\", thresh=1e-6):\n    \"\"\"\n    Compute theoretical AdaLoRA parameter budget:\n      - full_params: using max rank r_max for each module\n      - effective_params: using effective rank r_eff based on nonzero E\n    Returns (full_params, effective_params).\n\n    We deduplicate shared A/B/E by keying on (id(A), id(B), id(E)).\n    \"\"\"\n    full_params = 0\n    effective_params = 0\n    seen = set()\n\n    for _, module in model.named_modules():\n        if not is_adalora_linear(module):\n            continue\n\n        # available adapters on this module\n        adapters = list(getattr(module, \"lora_A\", {}).keys())\n        if len(adapters) == 0:\n            continue\n\n        name = adapter_name if adapter_name in adapters else adapters[0]\n\n        A = module.lora_A[name]\n        B = module.lora_B[name]\n        if hasattr(module, \"lora_E\") and name in module.lora_E:\n            E = module.lora_E[name].view(-1)\n        else:\n            # if no E, treat all directions as active\n            E = torch.ones(A.size(0), device=A.device)\n\n        key = (id(A), id(B), id(E))\n        if key in seen:\n            continue\n        seen.add(key)\n\n        r_max = A.size(0)\n        in_dim = A.size(1)\n        out_dim = B.size(0)\n\n        # full budget = max rank\n        full_params += r_max * (in_dim + out_dim + 1)\n\n        # effective rank: how many singular values are really used\n        r_eff = int((E.abs() > thresh).sum().item())\n        effective_params += r_eff * (in_dim + out_dim + 1)\n\n    return full_params, effective_params\n    \ndef train_eval_stage2(model, epochs=EPOCHS_STAGE2, batch_size=BATCH_SIZE_STAGE2, lr=1.2e-3,adalora_stage2=False,alpha=0):\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=data_collator,\n    )\n    valid_loader = DataLoader(\n        valid_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=data_collator,\n    )\n    test_loader = DataLoader(\n        test_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=data_collator,\n    )\n\n    unique_params = {}\n    for n, p in model.named_parameters():\n        if p.requires_grad:\n            unique_params[id(p)] = p\n    optim = torch.optim.AdamW(list(unique_params.values()), lr=lr, weight_decay=0.01)\n\n    total_steps = epochs * math.ceil(len(train_loader))\n    sched = get_linear_schedule_with_warmup(\n        optim,\n        num_warmup_steps=int(0.1 * total_steps),\n        num_training_steps=total_steps,\n    )\n\n    best_acc, best_state = 0.0, None\n    global_step = 0\n\n    def eval_loader(model, loader):\n        model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for batch in loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(\n                    input_ids=batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],\n                ).logits\n                pred = logits.argmax(-1)\n                correct += (pred == batch[\"labels\"]).sum().item()\n                total += pred.numel()\n        return correct / total\n\n    model.train()\n    for ep in range(epochs):\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = model(**batch)\n            loss = out.loss\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optim.step()\n\n            # AdaLoRA rank allocation in Stage-2\n            global_step += 1\n            model.base_model.update_and_allocate(global_step)\n\n            sched.step()\n            optim.zero_grad()\n\n        val_acc = eval_loader(model, valid_loader)\n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n        print(f\"Epoch {ep+1:02d} | val_acc={val_acc:.4f} | best={best_acc:.4f}\")\n        model.train()\n\n    model.load_state_dict(best_state)\n    model.to(device)\n    if alpha=90:\n        torch.save(best_state, \"best_adalora_drop90.pt\")\n    elif alpha=95:\n        torch.save(best_state, \"best_adalora_drop90.pt\")\n    else:\n        torch.save(best_state, \"best_adalora_full.pt\")\n    if adalora_stage2:\n        print(f\"[Stage-2] Best dev accuracy (from best_state): {best_acc:.4f}\")\n    else:\n        print(f\"Best dev accuracy (from best_state): {best_acc:.4f}\")\n\n    final_val_acc = eval_loader(model, valid_loader)\n    final_test_acc = eval_loader(model, test_loader)\n    if adalora_stage2:\n        print(f\"[Stage-2] Final dev accuracy (recomputed): {final_val_acc:.4f}\")\n        print(f\"[Stage-2] Test accuracy (best_state): {final_test_acc:.4f}\")\n    else:\n        print(f\"Final dev accuracy (recomputed): {final_val_acc:.4f}\")\n        print(f\"Test accuracy (best_state): {final_test_acc:.4f}\")\n\n    full_budget, eff_budget = compute_adalora_budget(model, adapter_name=\"default\", thresh=1e-6)\n    if adalora_stage2:\n        print(f\"[Stage-2] AdaLoRA-drop full parameter budget (max rank): {full_budget}\")\n        print(f\"[Stage-2] AdaLoRA-drop effective parameter budget (after pruning): {eff_budget}\")\n    else:\n        print(f\"AdaLoRA full parameter budget (max rank): {full_budget}\")\n        print(f\"AdaLoRA effective parameter budget (after pruning): {eff_budget}\")\n    if full_budget > 0:\n        if adalora_stage2:\n            print(f\"[Stage-2] AdaLoRA-drop compression ratio: {eff_budget / full_budget:.4f}\")\n        else:\n            print(f\"AdaLoRA compression ratio: {eff_budget / full_budget:.4f}\")\n\n    return final_val_acc, final_test_acc, eff_budget\n\n# AdaLoRA-drop @ 95% \nstage2_model_drop95 = build_stage2_model()\nshared_bank_95 = SharedAdaLoraBank(stage2_model_drop95).to(device)\nstage2_model_drop95.shared_bank = shared_bank_95\napply_sharing_with_bank(stage2_model_drop95, low95_ids, shared_bank_95)\n\nadalora95_val, adalora95_test, adalora95_eff = train_eval_stage2(\n    stage2_model_drop95, epochs=5, batch_size=64, lr=1.2e-3,adalora_stage2=True, alpha= 95\n)\nprint(\"[AdaLoRA-drop@95] dev_acc:\", adalora95_val, \"test_acc:\", adalora95_test)\n\n# AdaLoRA-drop @ 90% \nstage2_model_drop90 = build_stage2_model()\nshared_bank_90 = SharedAdaLoraBank(stage2_model_drop90).to(device)\nstage2_model_drop90.shared_bank = shared_bank_90\napply_sharing_with_bank(stage2_model_drop90, low90_ids, shared_bank_90)\n\nadalora90_val, adalora90_test, adalora90_eff = train_eval_stage2(\n    stage2_model_drop90, epochs=5, batch_size=64, lr=1.2e-3,adalora_stage2=True,alpha = 90\n)\nprint(\"[AdaLoRA-drop@90] dev_acc:\", adalora90_val, \"test_acc:\", adalora90_test)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:34:43.084678Z","iopub.execute_input":"2025-11-17T09:34:43.085462Z","iopub.status.idle":"2025-11-17T09:38:17.721968Z","shell.execute_reply.started":"2025-11-17T09:34:43.085437Z","shell.execute_reply":"2025-11-17T09:38:17.721144Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"[Stage-2] Shared parameters tied across 43 low-importance modules.\nEpoch 01 | val_acc=0.5640 | best=0.5640\nEpoch 02 | val_acc=0.8640 | best=0.8640\nEpoch 03 | val_acc=0.8780 | best=0.8780\nEpoch 04 | val_acc=0.8700 | best=0.8780\nEpoch 05 | val_acc=0.8820 | best=0.8820\n[Stage-2] Best dev accuracy (from best_state): 0.8820\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"[Stage-2] Final dev accuracy (recomputed): 0.8820\n[Stage-2] Test accuracy (best_state): 0.8744\n[Stage-2] AdaLoRA-drop full parameter budget (max rank): 433380\n[Stage-2] AdaLoRA-drop effective parameter budget (after pruning): 133698\n[Stage-2] AdaLoRA-drop compression ratio: 0.3085\n[AdaLoRA-drop@95] dev_acc: 0.882 test_acc: 0.874365739801096\n[Stage-2] Shared parameters tied across 49 low-importance modules.\nEpoch 01 | val_acc=0.6540 | best=0.6540\nEpoch 02 | val_acc=0.8020 | best=0.8020\nEpoch 03 | val_acc=0.8760 | best=0.8760\nEpoch 04 | val_acc=0.8680 | best=0.8760\nEpoch 05 | val_acc=0.8760 | best=0.8760\n[Stage-2] Best dev accuracy (from best_state): 0.8760\n[Stage-2] Final dev accuracy (recomputed): 0.8760\n[Stage-2] Test accuracy (best_state): 0.8736\n[Stage-2] AdaLoRA-drop full parameter budget (max rank): 336558\n[Stage-2] AdaLoRA-drop effective parameter budget (after pruning): 66846\n[Stage-2] AdaLoRA-drop compression ratio: 0.1986\n[AdaLoRA-drop@90] dev_acc: 0.876 test_acc: 0.8735538867464989\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"##############################################\n# BASELINE: Full RoBERTa-base\n##############################################\n\nfrom transformers import AutoModelForSequenceClassification, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader\nimport torch, math\n\n# assumes:\n#   - model_name = \"roberta-base\"\n#   - device already set\n#   - train_ds, valid_ds, test_ds already built\n#   - data_collator already defined (DataCollatorWithPadding)\n\n\ndef build_roberta_baseline():\n    \"\"\"\n    Plain RoBERTa-base + classification head.\n    All parameters are trainable (full fine-tuning).\n    \"\"\"\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=3,\n    )\n    model.to(device)\n    return model\n\n\ndef train_eval_roberta_baseline(model, epochs=5, batch_size=64, lr=2e-5):\n    \"\"\"\n    Train full RoBERTa-base on SciTail and return (val_acc, test_acc, trainable_params).\n    - epochs, batch_size can be matched to your AdaLoRA run.\n    - lr=2e-5 is a typical full FT LR; if you REALLY want\n      'same everything' as AdaLoRA, set lr=1.2e-3 (but that may be unstable).\n    \"\"\"\n    # DataLoaders (use same collator as your AdaLoRA code)\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=data_collator,\n    )\n    valid_loader = DataLoader(\n        valid_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=data_collator,\n    )\n    test_loader = DataLoader(\n        test_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=data_collator,\n    )\n\n    # Optimizer on ALL model params\n    optim = torch.optim.AdamW(\n        model.parameters(),\n        lr=lr,\n        weight_decay=0.01,\n    )\n\n    # Scheduler (same style as your AdaLoRA code)\n    total_steps = epochs * math.ceil(len(train_loader))\n    sched = get_linear_schedule_with_warmup(\n        optim,\n        num_warmup_steps=int(0.1 * total_steps),\n        num_training_steps=total_steps,\n    )\n\n    # Parameter counts for comparison\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"[Baseline] Total params:     {total_params}\")\n    print(f\"[Baseline] Trainable params: {trainable_params}\")\n\n    def eval_loader(m, loader):\n        m.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for batch in loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                # only pass what we need for logits\n                logits = m(\n                    input_ids=batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],\n                ).logits\n                pred = logits.argmax(dim=-1)\n                correct += (pred == batch[\"labels\"]).sum().item()\n                total += pred.numel()\n        return correct / total\n\n    best_acc, best_state = 0.0, None\n\n    # -------- TRAINING LOOP --------\n    for ep in range(epochs):\n        model.train()\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n\n            # model will use \"labels\" to compute loss\n            out = model(**batch)\n            loss = out.loss\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optim.step()\n            sched.step()\n            optim.zero_grad()\n\n        # validation accuracy\n        val_acc = eval_loader(model, valid_loader)\n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n\n        print(f\"[Baseline][Epoch {ep+1:02d}] val_acc={val_acc:.4f} | best={best_acc:.4f}\")\n\n    # restore best model\n    if best_state is not None:\n        model.load_state_dict(best_state)\n        model.to(device)\n        torch.save(best_state, \"best_baseline.pt\")\n\n    # final dev & test accuracy\n    final_val_acc = eval_loader(model, valid_loader)\n    final_test_acc = eval_loader(model, test_loader)\n    print(f\"[Baseline] Final dev accuracy:  {final_val_acc:.4f}\")\n    print(f\"[Baseline] Test accuracy:       {final_test_acc:.4f}\")\n\n    return final_val_acc, final_test_acc, trainable_params\n\n\n##############################################\n# Run baseline and compare to your AdaLoRA\n##############################################\n\nbaseline_model = build_roberta_baseline()\n\n# choose lr:\n# - 2e-5 is safer for full FT\n# - 1.2e-3 if you insist on exactly same LR as your AdaLoRA code\nbaseline_val_acc, baseline_test_acc, baseline_params = train_eval_roberta_baseline(\n    baseline_model,\n    epochs=5,\n    batch_size=64,\n    lr=2e-5,\n)\n\nprint(\"\\n===== COMPARISON =====\")\nprint(f\"Baseline dev acc     : {baseline_val_acc:.4f}\")\nprint(f\"Baseline test acc    : {baseline_test_acc:.4f}\")\n# assuming you already have these from your AdaLoRA run:\n#   best_val_acc, test_acc, adalora_eff_params\n# print(f\"AdaLoRA dev acc      : {best_val_acc:.4f}\")\n# print(f\"AdaLoRA test acc     : {test_acc:.4f}\")\n# print(f\"Baseline params      : {baseline_params}\")\n# print(f\"AdaLoRA eff. params  : {adalora_eff_params}\")\n# print(f\"Compression ratio    : {adalora_eff_params / baseline_params:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:38:17.723442Z","iopub.execute_input":"2025-11-17T09:38:17.724301Z","iopub.status.idle":"2025-11-17T09:40:05.891725Z","shell.execute_reply.started":"2025-11-17T09:38:17.724275Z","shell.execute_reply":"2025-11-17T09:40:05.890920Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"[Baseline] Total params:     124647939\n[Baseline] Trainable params: 124647939\n[Baseline][Epoch 01] val_acc=0.8560 | best=0.8560\n[Baseline][Epoch 02] val_acc=0.8900 | best=0.8900\n[Baseline][Epoch 03] val_acc=0.8800 | best=0.8900\n[Baseline][Epoch 04] val_acc=0.8920 | best=0.8920\n[Baseline][Epoch 05] val_acc=0.8920 | best=0.8920\n[Baseline] Final dev accuracy:  0.8920\n[Baseline] Test accuracy:       0.8863\n\n===== COMPARISON =====\nBaseline dev acc     : 0.8920\nBaseline test acc    : 0.8863\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# AdaLoRA-everywhere: no sharing, no dropping, rank allocated independently\nstage2_model_adalora_full = build_stage2_model()\n\nadalora_full_val, adalora_full_test, adalora_full_eff = train_eval_stage2(\n    stage2_model_adalora_full, epochs=5, batch_size=64, lr=1.2e-3, alpha = 0\n)\nprint(\"[AdaLoRA-all] dev_acc:\", adalora_full_val, \"test_acc:\", adalora_full_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:40:05.892672Z","iopub.execute_input":"2025-11-17T09:40:05.892965Z","iopub.status.idle":"2025-11-17T09:42:00.790145Z","shell.execute_reply.started":"2025-11-17T09:40:05.892946Z","shell.execute_reply":"2025-11-17T09:42:00.789419Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | val_acc=0.8080 | best=0.8080\nEpoch 02 | val_acc=0.8780 | best=0.8780\nEpoch 03 | val_acc=0.8960 | best=0.8960\nEpoch 04 | val_acc=0.8960 | best=0.8960\nEpoch 05 | val_acc=0.8900 | best=0.8960\nBest dev accuracy (from best_state): 0.8960\nFinal dev accuracy (recomputed): 0.8960\nTest accuracy (best_state): 0.8805\nAdaLoRA full parameter budget (max rank): 995760\nAdaLoRA effective parameter budget (after pruning): 682272\nAdaLoRA compression ratio: 0.6852\n[AdaLoRA-all] dev_acc: 0.896 test_acc: 0.8804546377105744\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from peft import LoraConfig\n# =========================\n# LoRA CONFIG (used for both LoRA-drop and LoRA-all)\n# =========================\nLORA_CFG = dict(\n    r=4,\n    lora_alpha=16,\n    lora_dropout=0.0,\n    bias=\"none\",\n    task_type=TaskType.SEQ_CLS,\n    target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n)\n\ndef build_lora_stage1_model():\n    base = AutoModelForSequenceClassification.from_pretrained(\n        model_name, num_labels=3\n    ).to(device)\n    lora_cfg = LoraConfig(**LORA_CFG)\n    model = get_peft_model(base, lora_cfg)\n    model.print_trainable_parameters()\n    return model\n\ndef build_lora_stage2_model():\n    base = AutoModelForSequenceClassification.from_pretrained(\n        model_name, num_labels=3\n    ).to(device)\n    lora_cfg = LoraConfig(**LORA_CFG)\n    model = get_peft_model(base, lora_cfg)\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:42:00.791710Z","iopub.execute_input":"2025-11-17T09:42:00.791994Z","iopub.status.idle":"2025-11-17T09:42:00.797296Z","shell.execute_reply.started":"2025-11-17T09:42:00.791975Z","shell.execute_reply":"2025-11-17T09:42:00.796544Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def train_for_importance_lora(\n    peft_model,\n    alpha=ALPHA_STAGE1,\n    epochs=EPOCHS_STAGE1,\n    lr=1.2e-3,\n    batch_size=BATCH_SIZE_STAGE1,\n):\n    n = len(train_ds)\n    k = max(1, int(alpha * n))\n    idx = list(range(n))\n    random.shuffle(idx); idx = idx[:k]\n    small = train_ds.select(idx)\n\n    dl = DataLoader(\n        small,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=data_collator,\n        drop_last=False,\n    )\n\n    unique_params = {}\n    for n, p in peft_model.named_parameters():\n        if p.requires_grad:\n            unique_params[id(p)] = p\n    optim = torch.optim.AdamW(list(unique_params.values()), lr=lr)\n\n    total_steps = epochs * math.ceil(len(dl))\n    sched = get_linear_schedule_with_warmup(\n        optim,\n        num_warmup_steps=int(0.1 * total_steps),\n        num_training_steps=total_steps,\n    )\n\n    energy, hooks = make_energy_meter(peft_model)\n    peft_model.train()\n\n    for ep in range(epochs):\n        for batch in dl:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = peft_model(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                labels=batch[\"labels\"],\n            )\n            loss = out.loss\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(peft_model.parameters(), 1.0)\n            optim.step()\n            sched.step()\n            optim.zero_grad()\n\n    remove_hooks(hooks)\n\n    total_e = sum(energy.values()) + 1e-12\n    importance = []\n    for (full_name, group, tag), val in energy.items():\n        importance.append({\n            \"name\": full_name, \"group\": group, \"tag\": tag,\n            \"energy\": val, \"importance\": val / total_e,\n        })\n    importance.sort(key=lambda d: d[\"importance\"], reverse=True)\n    return importance\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:42:00.798238Z","iopub.execute_input":"2025-11-17T09:42:00.798611Z","iopub.status.idle":"2025-11-17T09:42:00.817727Z","shell.execute_reply.started":"2025-11-17T09:42:00.798587Z","shell.execute_reply":"2025-11-17T09:42:00.817052Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class SharedLoraBank(nn.Module):\n    \"\"\"\n    LoRA version: share A/B across low-importance modules in each group.\n    Works whether lora_A / lora_B are nn.Linear modules or raw tensors.\n    \"\"\"\n    def __init__(self, model, adapter_name=\"default\"):\n        super().__init__()\n        self.adapter_name = adapter_name\n\n        # pick one exemplar per group to get shapes\n        exemplars = {}\n        for name, m in model.named_modules():\n            group, tag = classify_module_name(name)\n            if group and group not in exemplars and is_adalora_linear(m) and len(m.lora_A) > 0:\n                exemplars[group] = m\n\n        for group, m in exemplars.items():\n            adapter = self.adapter_name\n            if adapter not in m.lora_A:\n                adapter = next(iter(m.lora_A.keys()))\n\n            A_obj = m.lora_A[adapter]\n            B_obj = m.lora_B[adapter]\n\n            # unwrap weights if they are Linear modules\n            A_w = A_obj.weight if isinstance(A_obj, nn.Linear) else A_obj\n            B_w = B_obj.weight if isinstance(B_obj, nn.Linear) else B_obj\n\n            r, in_dim = A_w.shape\n            out_dim, r2 = B_w.shape\n            assert r == r2\n\n            # shared parameters\n            A_param = nn.Parameter(torch.empty_like(A_w))\n            B_param = nn.Parameter(torch.empty_like(B_w))\n            nn.init.kaiming_uniform_(A_param, a=math.sqrt(5))\n            nn.init.zeros_(B_param)\n\n            self.register_parameter(f\"{group}_A\", A_param)\n            self.register_parameter(f\"{group}_B\", B_param)\n\n    def tie_into(self, module, group):\n        \"\"\"\n        Tie this module's LoRA A/B weights to the shared bank.\n        \"\"\"\n        adapter = self.adapter_name\n        if adapter not in module.lora_A:\n            adapter = next(iter(module.lora_A.keys()))\n\n        shared_A = getattr(self, f\"{group}_A\")\n        shared_B = getattr(self, f\"{group}_B\")\n\n        A_obj = module.lora_A[adapter]\n        B_obj = module.lora_B[adapter]\n\n        # If A/B are Linear modules, assign to their .weight;\n        # otherwise, replace the stored tensor directly.\n        if isinstance(A_obj, nn.Linear):\n            A_obj.weight = shared_A\n        else:\n            module.lora_A[adapter] = shared_A\n\n        if isinstance(B_obj, nn.Linear):\n            B_obj.weight = shared_B\n        else:\n            module.lora_B[adapter] = shared_B\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:42:00.818432Z","iopub.execute_input":"2025-11-17T09:42:00.818610Z","iopub.status.idle":"2025-11-17T09:42:00.833435Z","shell.execute_reply.started":"2025-11-17T09:42:00.818596Z","shell.execute_reply":"2025-11-17T09:42:00.832795Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def train_eval_stage2_lora(model, epochs=EPOCHS_STAGE2, batch_size=BATCH_SIZE_STAGE2, lr=1.2e-3,lora_stage2=False):\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=data_collator,\n    )\n    valid_loader = DataLoader(\n        valid_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=data_collator,\n    )\n    test_loader = DataLoader(\n        test_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=data_collator,\n    )\n\n    unique_params = {}\n    for n, p in model.named_parameters():\n        if p.requires_grad:\n            unique_params[id(p)] = p\n    optim = torch.optim.AdamW(list(unique_params.values()), lr=lr, weight_decay=0.01)\n\n    trainable_params = sum(p.numel() for p in unique_params.values())\n    if lora_stage2:\n        print(f\"[LoRA-drop-Stage2] Trainable params (dedup): {trainable_params}\")\n    else:\n        print(f\"[LoRA] Trainable params (dedup): {trainable_params}\")\n\n    total_steps = epochs * math.ceil(len(train_loader))\n    sched = get_linear_schedule_with_warmup(\n        optim,\n        num_warmup_steps=int(0.1 * total_steps),\n        num_training_steps=total_steps,\n    )\n\n    best_acc, best_state = 0.0, None\n\n    def eval_loader(model, loader):\n        model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for batch in loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(\n                    input_ids=batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],\n                ).logits\n                pred = logits.argmax(-1)\n                correct += (pred == batch[\"labels\"]).sum().item()\n                total += pred.numel()\n        return correct / total\n\n    model.train()\n    for ep in range(epochs):\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            out = model(**batch)\n            loss = out.loss\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optim.step()\n            sched.step()\n            optim.zero_grad()\n\n        val_acc = eval_loader(model, valid_loader)\n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n        if lora_stage2:\n            print(f\"[LoRA-drop-Stage2][Epoch {ep+1:02d}] val_acc={val_acc:.4f} | best={best_acc:.4f}\")\n        else:\n            print(f\"[LoRA][Epoch {ep+1:02d}] val_acc={val_acc:.4f} | best={best_acc:.4f}\")\n\n        model.train()\n\n    model.load_state_dict(best_state)\n    model.to(device)\n    if lora_stage2:\n        torch.save(best_state, \"best_loradrop.pt\")\n    else:\n        torch.save(best_state, \"best_lora.pt\")\n    final_val_acc = eval_loader(model, valid_loader)\n    final_test_acc = eval_loader(model, test_loader)\n    if lora_stage2:\n        print(f\"[LoRA-drop-Stage2] Final dev accuracy:  {final_val_acc:.4f}\")\n        print(f\"[LoRA-drop-Stage2] Test accuracy:       {final_test_acc:.4f}\")\n    else:\n        print(f\"[LoRA] Final dev accuracy:  {final_val_acc:.4f}\")\n        print(f\"[LoRA] Test accuracy:       {final_test_acc:.4f}\")\n\n    return final_val_acc, final_test_acc, trainable_params\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:42:00.834123Z","iopub.execute_input":"2025-11-17T09:42:00.834535Z","iopub.status.idle":"2025-11-17T09:42:00.854298Z","shell.execute_reply.started":"2025-11-17T09:42:00.834513Z","shell.execute_reply":"2025-11-17T09:42:00.853553Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# ----- LoRA-drop @ 95% -----\nlora_stage1_model = build_lora_stage1_model()\nimportance_lora = train_for_importance_lora(\n    lora_stage1_model, alpha=0.10, epochs=3, lr=1.2e-3, batch_size=64\n)\nimportant_lora95_ids, low_lora95_ids, cum_lora95 = split_importance(importance_lora, 0.95)\nprint(f\"[LoRA-Stage1@0.95] selected {len(important_lora95_ids)} important, {len(low_lora95_ids)} low; cum={cum_lora95:.3f}\")\n\nstage2_lora_drop95 = build_lora_stage2_model()\nshared_lora_bank95 = SharedLoraBank(stage2_lora_drop95).to(device)\nstage2_lora_drop95.shared_bank = shared_lora_bank95\napply_sharing_with_bank(stage2_lora_drop95, low_lora95_ids, shared_lora_bank95)\n\nlora_drop95_val, lora_drop95_test, lora_drop95_params = train_eval_stage2_lora(\n    stage2_lora_drop95, epochs=5, batch_size=64, lr=1.2e-3, lora_stage2 = True\n)\nprint(\"[LoRA-drop@95] dev_acc:\", lora_drop95_val, \"test_acc:\", lora_drop95_test)\n\n# ----- LoRA-everywhere -----\nstage2_lora_full = build_lora_stage2_model()\nlora_full_val, lora_full_test, lora_full_params = train_eval_stage2_lora(\n    stage2_lora_full, epochs=5, batch_size=64, lr=1.2e-3\n)\nprint(\"[LoRA-all] dev_acc:\", lora_full_val, \"test_acc:\", lora_full_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T09:47:23.473076Z","iopub.execute_input":"2025-11-17T09:47:23.473433Z","iopub.status.idle":"2025-11-17T09:50:29.712256Z","shell.execute_reply.started":"2025-11-17T09:47:23.473411Z","shell.execute_reply":"2025-11-17T09:50:29.711511Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,256,451 || all params: 125,904,390 || trainable%: 0.9979\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"[LoRA-Stage1@0.95] selected 39 important, 33 low; cum=0.952\n[Stage-2] Shared parameters tied across 33 low-importance modules.\n[LoRA-drop-Stage2] Trainable params (dedup): 1062915\n[LoRA-drop-Stage2][Epoch 01] val_acc=0.8820 | best=0.8820\n[LoRA-drop-Stage2][Epoch 02] val_acc=0.8880 | best=0.8880\n[LoRA-drop-Stage2][Epoch 03] val_acc=0.9040 | best=0.9040\n[LoRA-drop-Stage2][Epoch 04] val_acc=0.8920 | best=0.9040\n[LoRA-drop-Stage2][Epoch 05] val_acc=0.9060 | best=0.9060\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"[LoRA-drop-Stage2] Final dev accuracy:  0.9060\n[LoRA-drop-Stage2] Test accuracy:       0.8900\n[LoRA-drop@95] dev_acc: 0.906 test_acc: 0.8899939111020905\n[LoRA] Trainable params (dedup): 1256451\n[LoRA][Epoch 01] val_acc=0.8560 | best=0.8560\n[LoRA][Epoch 02] val_acc=0.8920 | best=0.8920\n[LoRA][Epoch 03] val_acc=0.8840 | best=0.8920\n[LoRA][Epoch 04] val_acc=0.9000 | best=0.9000\n[LoRA][Epoch 05] val_acc=0.9100 | best=0.9100\n[LoRA] Final dev accuracy:  0.9100\n[LoRA] Test accuracy:       0.8939\n[LoRA-all] dev_acc: 0.91 test_acc: 0.8938502131114269\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import pandas as pd\n\n# Create DataFrame with your variables\ndf = pd.DataFrame({\n    'Model': [\n        'AdaLoRA-drop@95',\n        'AdaLoRA-drop@90', \n        'Baseline',\n        'AdaLoRA-all',\n        'LoRA-drop@95',\n        'LoRA-all'\n    ],\n    'Parameters': [\n        133698,\n        66846,\n        124647939,\n        682272,  \n        1062915,\n        1256451\n    ],\n    'Test Accuracy': [\n        adalora95_test,\n        adalora90_test,\n        baseline_test_acc,\n        adalora_full_test,\n        lora_drop95_test,\n        lora_full_test\n    ]\n})\n\n# Format for clean display\ndf['Parameters'] = df['Parameters'].apply(\n    lambda x: f\"{x:,}\" if pd.notna(x) else \"N/A\"\n)\ndf['Test Accuracy'] = df['Test Accuracy'].apply(lambda x: f\"{x:.4f}\")\n\n# Display without index\nprint(df.to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:04:15.238635Z","iopub.execute_input":"2025-11-17T10:04:15.239237Z","iopub.status.idle":"2025-11-17T10:04:15.247250Z","shell.execute_reply.started":"2025-11-17T10:04:15.239214Z","shell.execute_reply":"2025-11-17T10:04:15.246514Z"}},"outputs":[{"name":"stdout","text":"          Model  Parameters Test Accuracy\nAdaLoRA-drop@95     133,698        0.8744\nAdaLoRA-drop@90      66,846        0.8736\n       Baseline 124,647,939        0.8863\n    AdaLoRA-all     682,272        0.8805\n   LoRA-drop@95   1,062,915        0.8900\n       LoRA-all   1,256,451        0.8939\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}